{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b32258",
   "metadata": {},
   "source": [
    "# Dueling Deep Q-Network (DQN) using PyTorch\n",
    "Environment: LunarLander-v2\n",
    "\n",
    "### Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "555ed7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f1e21d0eb50>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from pyvirtualdisplay import Display\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    \n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312da450",
   "metadata": {},
   "source": [
    "## Specify the Environment, and Explore the State and Action Spaces\n",
    "* Let's begin with an initializing the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "05fa0d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "State shape:  (4,)\n",
      "Action space:  Discrete(2)\n",
      "Number of actions:  2\n"
     ]
    }
   ],
   "source": [
    "# Create an environment\n",
    "env = gym.make('CartPole-v1')\n",
    "print('State space: ', env.observation_space)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "\n",
    "print('Action space: ', env.action_space)\n",
    "print('Number of actions: ', env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b9996",
   "metadata": {},
   "source": [
    "### Implement Q-Network\n",
    "Building the Network: Actor (policy) Model\n",
    "input_size = state_size\n",
    "output_size = action_size\n",
    "using same seed\n",
    "hidden_layers: fc1, fc2\n",
    "\n",
    "<!-- Define Layer of model: [FC-RELU-FC-RELU-FC] -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "514d85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MaxQNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "\n",
    "        super(MaxQNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        # Add the first laer, input to hidden layer\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        # Add more hidden layer\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "\n",
    "        # State-value V\n",
    "        self.V = nn.Linear(fc2_units, 1)\n",
    "        \n",
    "        # Advantage function A\n",
    "        self.A = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, state):\n",
    "\n",
    "        features = nn.Sequential(\n",
    "            self.fc1,\n",
    "            nn.ReLU(),\n",
    "            self.fc2,\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        V = self.V(features(state))\n",
    "        A = self.A(features(state))\n",
    "        A_max = A.max(dim=1, keepdim=True)[0]\n",
    "        A_max_expanded = A_max.expand(-1, A.size(1))\n",
    "\n",
    "        return V + (A - A_max_expanded)\n",
    "    \n",
    "    \n",
    "class MeanQNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "\n",
    "        super(MeanQNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.V = nn.Linear(fc2_units, 1)\n",
    "        self.A = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, state):\n",
    "\n",
    "        features = nn.Sequential(\n",
    "            self.fc1,\n",
    "            nn.ReLU(),\n",
    "            self.fc2,\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        V = self.V(features(state))\n",
    "        A = self.A(features(state))\n",
    "        return V + (A - A.mean(dim=1, keepdim=True))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "189dba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "\n",
    "        self.acion_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device) # gpu\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        # return D\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbcb7fe",
   "metadata": {},
   "source": [
    "### Implement agent\n",
    "* Agent(state_size=8, action_size=4, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6dd74428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HYPERPARAMETERS\n",
    "LR = 2.5e-3                # learning rate # 2.5e-3\n",
    "BUFFER_SIZE = int(1e5)   # replay buffer size N # 1e5\n",
    "BATCH_SIZE = 128          # minibatch size # 128\n",
    "UPDATE_EVERY = 5         # how often to update the network # 5\n",
    "GAMMA = 0.99             # Discount factor # 0.99\n",
    "TAU = 1e-2               # for soft update of target parameters # 1e-2\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, DuelingDQN):\n",
    "       \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.qnetwork_local = DuelingDQN(state_size, action_size, seed).to(device) \n",
    "        self.qnetwork_target = DuelingDQN(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR) \n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.t_step =(self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > BATCH_SIZE: \n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "                \n",
    "    def act(self, state, eps=0.1):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        ''' Epsilon-greedy action selection (Already Present) '''\n",
    "        action_values = torch.tensor(action_values, dtype=torch.float32)\n",
    "        action_probs = torch.softmax(action_values, dim=1)\n",
    "        chosen_action = torch.multinomial(action_probs, 1).item()\n",
    "        return chosen_action\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU) \n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "91021fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(env, agent, n_episodes = 500, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "\n",
    "    scores_window = deque(maxlen=100)\n",
    "    scores_array = []\n",
    "    eps = eps_start\n",
    "    reward_sat=False\n",
    "    reahced_flag = False\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state = env.reset()[0]\n",
    "        # print(state)\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            if not reward_sat:\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    \n",
    "        scores_window.append(score)\n",
    "        scores_array.append(np.mean(scores_window))\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= 475 and not reahced_flag:\n",
    "            reahced_flag = True\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            reward_sat=True\n",
    "    return scores_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "233af488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deuling dqn\n",
    "def run_agent(env, DuelingDQN, state_shape, action_shape):\n",
    "    scores_final = []\n",
    "    for i in range(5):\n",
    "        # Assuming env, TutorialAgent, and other required variables are defined\n",
    "        agent = Agent(state_size=state_shape, action_size=action_shape, seed=np.random.randint(1,100), DuelingDQN = DuelingDQN)\n",
    "        scores_array = dqn(env, agent)\n",
    "        scores_final.append(scores_array)\n",
    "    scores = np.array(scores_final)\n",
    "    scores = np.mean(scores, axis=0)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e11784ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment : CartPole-v1\n",
      "Episode 12\tAverage Score: 16.83"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_322266/2929858312.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action_values = torch.tensor(action_values, dtype=torch.float32)\n",
      "/home/adi/miniconda3/envs/rl/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 21.09\n",
      "Episode 200\tAverage Score: 46.55\n",
      "Episode 300\tAverage Score: 185.44\n",
      "Episode 400\tAverage Score: 187.77\n",
      "Episode 445\tAverage Score: 481.65\n",
      "Environment solved in 445 episodes!\tAverage Score: 481.65\n",
      "Episode 500\tAverage Score: 902.44\n",
      "Episode 100\tAverage Score: 20.32\n",
      "Episode 200\tAverage Score: 35.09\n",
      "Episode 300\tAverage Score: 214.71\n",
      "Episode 400\tAverage Score: 222.84\n",
      "Episode 465\tAverage Score: 483.04\n",
      "Environment solved in 465 episodes!\tAverage Score: 483.04\n",
      "Episode 500\tAverage Score: 744.35\n",
      "Episode 100\tAverage Score: 21.38\n",
      "Episode 200\tAverage Score: 42.22\n",
      "Episode 300\tAverage Score: 162.59\n",
      "Episode 400\tAverage Score: 153.26\n",
      "Episode 500\tAverage Score: 221.04\n",
      "Episode 100\tAverage Score: 20.75\n",
      "Episode 200\tAverage Score: 36.80\n",
      "Episode 300\tAverage Score: 231.42\n",
      "Episode 375\tAverage Score: 475.57\n",
      "Environment solved in 375 episodes!\tAverage Score: 475.57\n",
      "Episode 400\tAverage Score: 611.57\n",
      "Episode 500\tAverage Score: 835.15\n",
      "Episode 100\tAverage Score: 24.02\n",
      "Episode 200\tAverage Score: 75.99\n",
      "Episode 300\tAverage Score: 184.67\n",
      "Episode 400\tAverage Score: 189.31\n",
      "Episode 472\tAverage Score: 476.95\n",
      "Environment solved in 472 episodes!\tAverage Score: 476.95\n",
      "Episode 500\tAverage Score: 695.66\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "print(\"Environment : CartPole-v1\")\n",
    "average_mean_cartpole = run_agent(env=env, DuelingDQN=MeanQNetwork, state_shape = env.observation_space.shape[0], action_shape = env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "222a698b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment : CartPole-v1\n",
      "Episode 8\tAverage Score: 15.25"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_322266/2929858312.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action_values = torch.tensor(action_values, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 22.91\n",
      "Episode 200\tAverage Score: 39.96\n",
      "Episode 300\tAverage Score: 179.20\n",
      "Episode 400\tAverage Score: 167.40\n",
      "Episode 500\tAverage Score: 212.54\n",
      "Episode 100\tAverage Score: 22.79\n",
      "Episode 200\tAverage Score: 43.09\n",
      "Episode 300\tAverage Score: 203.63\n",
      "Episode 400\tAverage Score: 242.96\n",
      "Episode 434\tAverage Score: 480.41\n",
      "Environment solved in 434 episodes!\tAverage Score: 480.41\n",
      "Episode 500\tAverage Score: 954.72\n",
      "Episode 100\tAverage Score: 21.56\n",
      "Episode 200\tAverage Score: 35.23\n",
      "Episode 300\tAverage Score: 178.08\n",
      "Episode 400\tAverage Score: 203.79\n",
      "Episode 453\tAverage Score: 480.92\n",
      "Environment solved in 453 episodes!\tAverage Score: 480.92\n",
      "Episode 500\tAverage Score: 847.80\n",
      "Episode 100\tAverage Score: 20.81\n",
      "Episode 200\tAverage Score: 67.58\n",
      "Episode 300\tAverage Score: 178.34\n",
      "Episode 400\tAverage Score: 215.23\n",
      "Episode 457\tAverage Score: 480.14\n",
      "Environment solved in 457 episodes!\tAverage Score: 480.14\n",
      "Episode 500\tAverage Score: 799.69\n",
      "Episode 100\tAverage Score: 21.96\n",
      "Episode 200\tAverage Score: 51.67\n",
      "Episode 300\tAverage Score: 228.36\n",
      "Episode 377\tAverage Score: 476.03\n",
      "Environment solved in 377 episodes!\tAverage Score: 476.03\n",
      "Episode 400\tAverage Score: 649.15\n",
      "Episode 500\tAverage Score: 1000.00\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "print(\"Environment : CartPole-v1\")\n",
    "average_max_cartpole = run_agent(env=env, DuelingDQN=MaxQNetwork, state_shape = env.observation_space.shape[0], action_shape = env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "77215ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/home/adi/courses/reinforcement_learning/assignment/Assignment_2/RL_Assignment_2_ED21B005_ED21B046/mean_cartpole.npy\", average_mean_cartpole)\n",
    "np.save(\"/home/adi/courses/reinforcement_learning/assignment/Assignment_2/RL_Assignment_2_ED21B005_ED21B046/max_cartpole.npy\", average_max_cartpole)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
