{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import gym\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "import tensorflow as tf\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n",
      "1\n",
      "----\n",
      "(array([-0.00919449,  0.01528829, -0.04272228, -0.03958981], dtype=float32), {})\n",
      "----\n",
      "1\n",
      "----\n",
      "[-0.00888873  0.21099602 -0.04351408 -0.34544006]\n",
      "1.0\n",
      "False\n",
      "{}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "# env.seed(0)\n",
    "\n",
    "state_shape = env.observation_space.shape[0]\n",
    "no_of_actions = env.action_space.n\n",
    "\n",
    "print(state_shape)\n",
    "print(no_of_actions)\n",
    "print(env.action_space.sample())\n",
    "print(\"----\")\n",
    "\n",
    "'''\n",
    "# Understanding State, Action, Reward Dynamics\n",
    "\n",
    "The agent decides an action to take depending on the state.\n",
    "\n",
    "The Environment keeps a variable specifically for the current state.\n",
    "- Everytime an action is passed to the environment, it calculates the new state and updates the current state variable.\n",
    "- It returns the new current state and reward for the agent to take the next action\n",
    "\n",
    "'''\n",
    "\n",
    "state = env.reset()\n",
    "''' This returns the initial state (when environment is reset) '''\n",
    "\n",
    "print(state)\n",
    "print(\"----\")\n",
    "\n",
    "action = env.action_space.sample()\n",
    "''' We take a random action now '''\n",
    "\n",
    "print(action)\n",
    "print(\"----\")\n",
    "\n",
    "next_state, reward, done, _, info = env.step(action)\n",
    "''' env.step is used to calculate new state and obtain reward based on old state and action taken  '''\n",
    "\n",
    "print(next_state)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)\n",
    "print(\"----\")\n",
    "\n",
    "TAU = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### DUELING DQN with MEAN ########################################\n",
    "class MeanDuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden, seed):\n",
    "        super(MeanDuelingDQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.value_stream = nn.Sequential(\n",
    "            # nn.Linear(128, 128),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        self.advantage = nn.Sequential(\n",
    "            # nn.Linear(128, 128),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(64, self.output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        features = self.feature_extraction(state)\n",
    "        values = self.value_stream(features)\n",
    "        advantages = self.advantage(features)\n",
    "        qvals = values + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "    \n",
    "        return qvals\n",
    "\n",
    "######################################################################################################\n",
    "    \n",
    "###################################### DUELING DQN with MAX ########################################\n",
    "\n",
    "class MaxDuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden, seed):\n",
    "        super(MaxDuelingDQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.value_stream = nn.Sequential(\n",
    "            # nn.Linear(128, 128),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        self.advantage = nn.Sequential(\n",
    "            # nn.Linear(128, 128),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(64, self.output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        features = self.feature_extraction(state)\n",
    "        values = self.value_stream(features)\n",
    "        advantages = self.advantage(features)\n",
    "        A_max = advantages.max(dim=1, keepdim=True)[0]\n",
    "        A_max_expanded = A_max.expand(-1, advantages.size(1))\n",
    "        qvals = values + (advantages - A_max_expanded)\n",
    "    \n",
    "        return qvals\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## REPLAY BUFFER #########################################\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, DuelingDQN, hidden = 128):\n",
    "\n",
    "        ''' Agent Environment Interaction '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        ''' Q-Network '''\n",
    "        self.qnetwork_local = DuelingDQN(state_size, action_size, hidden, seed).to(device)\n",
    "        self.qnetwork_target = DuelingDQN(state_size, action_size, hidden, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        ''' Replay memory '''\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "\n",
    "        ''' Save experience in replay memory '''\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        ''' If enough samples are available in memory, get random subset and learn '''\n",
    "        if len(self.memory) >= BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "        \"\"\" +Q TARGETS PRESENT \"\"\"\n",
    "        ''' Updating the Network every 'UPDATE_EVERY' steps taken '''\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "\n",
    "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        ''' Epsilon-greedy action selection (Already Present) '''\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ''' Get max predicted Q values (for next states) from target model'''\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        ''' Compute Q targets for current states '''\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        ''' Get expected Q values from local model '''\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        ''' Compute loss '''\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        ''' Minimize the loss '''\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        ''' Gradiant Clipping '''\n",
    "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
    "        for param in self.qnetwork_local.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        # self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU) \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def dqn(env, agent, threshold, n_episodes=500, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, tau = 0.1):\n",
    "    scores_window = deque(maxlen=100)\n",
    "    seed = np.random.randint(0, 100)\n",
    "    scores_array = []\n",
    "    eps = eps_start\n",
    "    reward_sat=False\n",
    "    reahced_flag = False\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state = env.reset(seed = seed)[0]\n",
    "        # print(state)\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            if not reward_sat:\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores_window.append(score)\n",
    "        scores_array.append(np.mean(scores_window))\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= threshold and not reahced_flag:\n",
    "            reahced_flag = True\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            reward_sat=True\n",
    "\n",
    "    return scores_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(env, DuelingDQN, state_shape, action_shape, threshold, max_t , hidden = 128):\n",
    "    # Trial run to check if the algorithm runs and saves the data\n",
    "    begin_time = datetime.datetime.now()\n",
    "\n",
    "    # Assuming env, TutorialAgent, and other required variables are defined\n",
    "    agent = Agent(state_size=state_shape, action_size=action_shape, seed=np.random.randint(1,100), DuelingDQN = DuelingDQN, hidden = hidden)\n",
    "    scores_array = dqn(env, agent, threshold, max_t=max_t)\n",
    "\n",
    "    time_taken = datetime.datetime.now() - begin_time\n",
    "    print(time_taken)\n",
    "\n",
    "    return scores_array, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_scores, mean_agent = run_agent(env, MeanDuelingDQN, state_shape, action_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_environment(env, DuelingDQN, threshold, hidden = 128, runs = 5, max_t = 1000):\n",
    "    scores_array = []\n",
    "    for run in range(runs):\n",
    "        print(f\"\\n Run : {run}\")\n",
    "        state_shape = env.observation_space.shape[0]\n",
    "        action_shape = env.action_space.n\n",
    "        scores, agent = run_agent(env, DuelingDQN, state_shape, action_shape, threshold, max_t, hidden=hidden)\n",
    "        scores_array.append(scores)\n",
    "    scores_array = np.array(scores_array)\n",
    "    average_scores = np.mean(scores_array, axis=0)\n",
    "    return average_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### ACROBOT MEAN #####################\n",
    "##################### HYPERPARAMETERS #########################\n",
    "BUFFER_SIZE = int(1e5)              # replay buffer size\n",
    "BATCH_SIZE = 130                    # minibatch size\n",
    "GAMMA = 0.99                        # discount factor\n",
    "LR = 0.0001328080938018766          # learning rate\n",
    "UPDATE_EVERY = 24                   # how often to update the network (When Q target is present)\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment : Acrobot-v1\n",
      "\n",
      " Run : 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -428.56\n",
      "Episode 200\tAverage Score: -184.74\n",
      "Episode 300\tAverage Score: -120.08\n",
      "Episode 400\tAverage Score: -103.54\n",
      "Episode 420\tAverage Score: -99.639\n",
      "Environment solved in 420 episodes!\tAverage Score: -99.63\n",
      "Episode 500\tAverage Score: -97.703\n",
      "0:09:11.815630\n",
      "\n",
      " Run : 1\n",
      "Episode 100\tAverage Score: -369.17\n",
      "Episode 200\tAverage Score: -154.88\n",
      "Episode 300\tAverage Score: -112.98\n",
      "Episode 391\tAverage Score: -99.996\n",
      "Environment solved in 391 episodes!\tAverage Score: -99.99\n",
      "Episode 400\tAverage Score: -99.81\n",
      "Episode 500\tAverage Score: -90.137\n",
      "0:07:42.756675\n",
      "\n",
      " Run : 2\n",
      "Episode 100\tAverage Score: -325.99\n",
      "Episode 200\tAverage Score: -153.48\n",
      "Episode 300\tAverage Score: -114.44\n",
      "Episode 379\tAverage Score: -99.991\n",
      "Environment solved in 379 episodes!\tAverage Score: -99.99\n",
      "Episode 400\tAverage Score: -98.174\n",
      "Episode 500\tAverage Score: -87.93\n",
      "0:04:42.632343\n",
      "\n",
      " Run : 3\n",
      "Episode 100\tAverage Score: -356.49\n",
      "Episode 200\tAverage Score: -159.30\n",
      "Episode 300\tAverage Score: -118.18\n",
      "Episode 400\tAverage Score: -102.46\n",
      "Episode 426\tAverage Score: -98.697\n",
      "Environment solved in 426 episodes!\tAverage Score: -98.69\n",
      "Episode 500\tAverage Score: -96.86\n",
      "0:05:01.055893\n",
      "\n",
      " Run : 4\n",
      "Episode 100\tAverage Score: -346.31\n",
      "Episode 200\tAverage Score: -158.13\n",
      "Episode 300\tAverage Score: -114.00\n",
      "Episode 400\tAverage Score: -106.42\n",
      "Episode 412\tAverage Score: -99.514\n",
      "Environment solved in 412 episodes!\tAverage Score: -99.51\n",
      "Episode 500\tAverage Score: -92.40\n",
      "0:04:35.614363\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "print(\"Environment : Acrobot-v1\")\n",
    "average_mean_acrobot = run_environment(env=env, DuelingDQN=MeanDuelingDQN, threshold=-100, max_t = 500, hidden=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### ACROBOT MAX #####################\n",
    "##################### HYPERPARAMETERS #########################\n",
    "BUFFER_SIZE = int(1e5)              # replay buffer size\n",
    "BATCH_SIZE = 124                    # minibatch size\n",
    "GAMMA = 0.99                        # discount factor\n",
    "LR = 0.0002798195586615115          # learning rate\n",
    "UPDATE_EVERY = 12                   # how often to update the network (When Q target is present)\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"mean_acrobot.npy\", average_mean_acrobot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment : Acrobot-v1\n",
      "\n",
      " Run : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adi/miniconda3/envs/rl/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -354.35\n",
      "Episode 200\tAverage Score: -152.83\n",
      "Episode 300\tAverage Score: -115.07\n",
      "Episode 400\tAverage Score: -108.36\n",
      "Episode 458\tAverage Score: -99.641\n",
      "Environment solved in 458 episodes!\tAverage Score: -99.64\n",
      "Episode 500\tAverage Score: -95.23\n",
      "0:05:39.313289\n",
      "\n",
      " Run : 1\n",
      "Episode 100\tAverage Score: -399.20\n",
      "Episode 200\tAverage Score: -165.02\n",
      "Episode 300\tAverage Score: -122.52\n",
      "Episode 400\tAverage Score: -104.15\n",
      "Episode 406\tAverage Score: -99.938\n",
      "Environment solved in 406 episodes!\tAverage Score: -99.93\n",
      "Episode 500\tAverage Score: -96.215\n",
      "0:03:09.638895\n",
      "\n",
      " Run : 2\n",
      "Episode 100\tAverage Score: -388.46\n",
      "Episode 200\tAverage Score: -164.84\n",
      "Episode 300\tAverage Score: -122.53\n",
      "Episode 400\tAverage Score: -103.82\n",
      "Episode 500\tAverage Score: -104.87\n",
      "0:03:32.699941\n",
      "\n",
      " Run : 3\n",
      "Episode 100\tAverage Score: -447.56\n",
      "Episode 200\tAverage Score: -161.98\n",
      "Episode 300\tAverage Score: -123.15\n",
      "Episode 400\tAverage Score: -107.20\n",
      "Episode 436\tAverage Score: -99.002\n",
      "Environment solved in 436 episodes!\tAverage Score: -99.00\n",
      "Episode 500\tAverage Score: -96.513\n",
      "0:03:30.467114\n",
      "\n",
      " Run : 4\n",
      "Episode 100\tAverage Score: -424.91\n",
      "Episode 200\tAverage Score: -179.03\n",
      "Episode 300\tAverage Score: -133.64\n",
      "Episode 400\tAverage Score: -118.25\n",
      "Episode 500\tAverage Score: -109.33\n",
      "0:03:53.730223\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "print(\"Environment : Acrobot-v1\")\n",
    "average_max_acrobot = run_environment(env=env, DuelingDQN=MaxDuelingDQN, threshold=-100, max_t = 500, hidden=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"max_acrobot.npy\", average_max_acrobot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
