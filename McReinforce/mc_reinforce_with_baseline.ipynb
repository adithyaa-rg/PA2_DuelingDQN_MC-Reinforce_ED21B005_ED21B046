{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import sys\n",
    "import random\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "import wandb\n",
    "from plyer import notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0 \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_actions)\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        action_scores = self.linear2(x)\n",
    "        state_values = self.linear3(x)\n",
    "        return F.softmax(action_scores, dim=1), state_values\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs, state_value = self.forward(Variable(state))\n",
    "        highest_prob_action = np.random.choice(self.num_actions, p=np.squeeze(probs.detach().numpy()))\n",
    "        log_prob = torch.log(probs.squeeze(0)[highest_prob_action])\n",
    "        return highest_prob_action, log_prob, state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards):\n",
    "    discounted_rewards = []\n",
    "    for t in range(len(rewards)):\n",
    "        Gt = 0 \n",
    "        pw = 0\n",
    "        for r in rewards[t:]:\n",
    "            Gt = Gt + GAMMA**pw * r\n",
    "            pw = pw + 1\n",
    "        discounted_rewards.append(Gt)\n",
    "    return discounted_rewards\n",
    "\n",
    "def update_policy(policy_network, rewards, log_probs, state_values):\n",
    "    discounted_rewards = compute_returns(rewards)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "    policy_gradient = []\n",
    "    value_loss = []\n",
    "    for log_prob, value, Gt in zip(log_probs, state_values, discounted_rewards):\n",
    "        advantage = Gt - value.item()\n",
    "        policy_gradient.append(-log_prob * advantage)\n",
    "    for i in range(0, len(state_values)-1): \n",
    "        value_loss.append(F.smooth_l1_loss(rewards[i]+GAMMA*state_values[i+1], state_values[i]))\n",
    "    policy_network.optimizer.zero_grad()\n",
    "    loss = torch.stack(policy_gradient).sum() + torch.stack(value_loss).sum()\n",
    "    loss.backward()\n",
    "    policy_network.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(environment, render=False, hidden_size=128, learning_rate=3e-4, max_episode_num=750, max_steps=1000):\n",
    "    env = gym.make(environment[\"name\"])\n",
    "    env.seed(random.randint(0,100))\n",
    "    policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.n, hidden_size, learning_rate).to(device)\n",
    "    reward_sat = False\n",
    "    all_rewards = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    for episode in range(max_episode_num):\n",
    "        state = env.reset() \n",
    "        log_probs = []\n",
    "        state_values = []\n",
    "        rewards = []\n",
    "        episode_reward = 0\n",
    "        for steps in range(max_steps):\n",
    "            if render:\n",
    "                env.render()\n",
    "            action, log_prob, state_value = policy_net.get_action(state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            episode_reward+=reward\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            state_values.append(state_value)\n",
    "            if done:\n",
    "                scores_window.append(episode_reward)\n",
    "                if not reward_sat:\n",
    "                    update_policy(policy_net, rewards, log_probs, state_values)\n",
    "                else:\n",
    "                    plot_post_sat-=1\n",
    "                all_rewards.append(episode_reward)\n",
    "                break\n",
    "            state = new_state\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tTotal Reward: {:.2f}\\tAverage Reward: {:.2f}'.format(episode, episode_reward, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= environment[\"avg_reward_threshold\"]:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Reward: {:.2f}'.format(episode, np.mean(scores_window)))\n",
    "            reward_sat=True\n",
    "    \n",
    "    env.close()\n",
    "    return all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(environment, learning_rates):\n",
    "    all_scores = []\n",
    "    for lr in learning_rates:\n",
    "        five_runs=[]\n",
    "        for _ in range(5):\n",
    "            print(\"Run \", _, \": \", lr)\n",
    "            scores = train(environment, False, 128, lr)\n",
    "            five_runs.append(scores)\n",
    "        all_scores.append(five_runs)\n",
    "    notification.notify(title=\"Run Complete\",\n",
    "                        message=\"Your rewards have been plotted\")\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment = {\"name\": 'Acrobot-v1', \"avg_reward_threshold\": -100}\n",
    "# # learning_rates = np.linspace(2e-3, 9e-3, 15)\n",
    "# learning_rates = [0.003]\n",
    "# all_scores_1 = run_agent(environment, learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = {\"name\": 'CartPole-v1', \"avg_reward_threshold\": 475}\n",
    "# learning_rates = np.linspace(2e-3, 9e-3, 15)\n",
    "learning_rates = [0.0025]\n",
    "all_scores_2 = run_agent(environment, learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"./plots/acrobot_without_baseline\", np.array(all_scores_1))\n",
    "np.save(\"./plots/cartpole_without_baseline\", np.array(all_scores_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
