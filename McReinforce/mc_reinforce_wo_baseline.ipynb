{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import sys\n",
    "import random\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "import wandb\n",
    "from plyer import notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "GAMMA = 0.99\n",
    "# learning_rate = 3e-4\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class NoBaselinePolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
    "        super(NoBaselinePolicyNetwork, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.softmax(self.linear2(x), dim=1)\n",
    "        return x \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = self.forward(Variable(state))\n",
    "        highest_prob_action = np.random.choice(self.num_actions, p=np.squeeze(probs.detach().numpy()))\n",
    "        log_prob = torch.log(probs.squeeze(0)[highest_prob_action])\n",
    "        return highest_prob_action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards):\n",
    "    discounted_rewards = []\n",
    "    for t in range(len(rewards)):\n",
    "        Gt = 0 \n",
    "        pw = 0\n",
    "        for r in rewards[t:]:\n",
    "            Gt = Gt + GAMMA**pw * r\n",
    "            pw = pw + 1\n",
    "        discounted_rewards.append(Gt)\n",
    "    return discounted_rewards\n",
    "\n",
    "def update_policy(policy_network, rewards, log_probs):\n",
    "    discounted_rewards = compute_returns(rewards)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "    policy_gradient = []\n",
    "    for log_prob, Gt in zip(log_probs, discounted_rewards):\n",
    "        policy_gradient.append(-log_prob * Gt)\n",
    "    policy_network.optimizer.zero_grad()\n",
    "    policy_gradient = torch.stack(policy_gradient).sum()\n",
    "    policy_gradient.backward()\n",
    "    policy_network.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(environment, render=False, hidden_size=128, learning_rate=3e-4, max_episode_num=750, max_steps=1000):\n",
    "    env = gym.make(environment[\"name\"])\n",
    "    env.seed(random.randint(0,100))\n",
    "    policy_net = NoBaselinePolicyNetwork(env.observation_space.shape[0], env.action_space.n, hidden_size, learning_rate).to(device)\n",
    "    reward_sat = False\n",
    "    all_rewards = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    for episode in range(max_episode_num):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        episode_reward = 0\n",
    "        for steps in range(max_steps):\n",
    "            if render:\n",
    "                env.render()\n",
    "            action, log_prob = policy_net.get_action(state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            episode_reward+=reward\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                scores_window.append(episode_reward)\n",
    "                if not reward_sat:\n",
    "                    update_policy(policy_net, rewards, log_probs)\n",
    "                all_rewards.append(episode_reward)\n",
    "                break\n",
    "            state = new_state\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tTotal Reward: {:.2f}\\tAverage Reward: {:.2f}'.format(episode, episode_reward, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= environment[\"avg_reward_threshold\"]:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Reward: {:.2f}'.format(episode, np.mean(scores_window)))\n",
    "            reward_sat=True\n",
    "    \n",
    "    env.close()\n",
    "    return all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(environment, learning_rates):\n",
    "    all_scores = []\n",
    "    for lr in learning_rates:\n",
    "        five_runs=[]\n",
    "        for _ in range(5):\n",
    "            print(\"Run \", _, \": \", lr)\n",
    "            scores = train(environment, False, 128, lr)\n",
    "            five_runs.append(scores)\n",
    "        all_scores.append(five_runs)\n",
    "    notification.notify(title=\"Run Complete\",\n",
    "                        message=\"Your rewards have been plotted\")\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment = {\"name\": 'Acrobot-v1', \"avg_reward_threshold\": -100}\n",
    "# # learning_rates = np.linspace(2e-3, 9e-3, 15)\n",
    "# learning_rates = [0.003]\n",
    "# all_scores_1 = run_agent(environment, learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = {\"name\": 'CartPole-v1', \"avg_reward_threshold\": 475}\n",
    "# learning_rates = np.linspace(2e-3, 9e-3, 15)\n",
    "learning_rates = [0.003283]\n",
    "all_scores_2 = run_agent(environment, learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"./plots/acrobot_without_baseline\", np.array(all_scores_1))\n",
    "# np.save(\"./plots/cartpole_without_baseline\", np.array(all_scores_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
